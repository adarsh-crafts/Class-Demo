{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 109, 'name': 'Wine', 'repository_url': 'https://archive.ics.uci.edu/dataset/109/wine', 'data_url': 'https://archive.ics.uci.edu/static/public/109/data.csv', 'abstract': 'Using chemical analysis to determine the origin of wines', 'area': 'Physics and Chemistry', 'tasks': ['Classification'], 'characteristics': ['Tabular'], 'num_instances': 178, 'num_features': 13, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1992, 'last_updated': 'Mon Aug 28 2023', 'dataset_doi': '10.24432/C5PC7J', 'creators': ['Stefan Aeberhard', 'M. Forina'], 'intro_paper': {'ID': 246, 'type': 'NATIVE', 'title': 'Comparative analysis of statistical pattern recognition methods in high dimensional settings', 'authors': 'S. Aeberhard, D. Coomans, O. Vel', 'venue': 'Pattern Recognition', 'year': 1994, 'journal': None, 'DOI': '10.1016/0031-3203(94)90145-7', 'URL': 'https://www.semanticscholar.org/paper/83dc3e4030d7b9fbdbb4bde03ce12ab70ca10528', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. \\r\\n\\r\\nI think that the initial data set had around 30 variables, but for some reason I only have the 13 dimensional version. I had a list of what the 30 or so variables were, but a.)  I lost it, and b.), I would not know which 13 variables are included in the set.\\r\\n\\r\\nThe attributes are (dontated by Riccardo Leardi, riclea@anchem.unige.it )\\r\\n1) Alcohol\\r\\n2) Malic acid\\r\\n3) Ash\\r\\n4) Alcalinity of ash  \\r\\n5) Magnesium\\r\\n6) Total phenols\\r\\n7) Flavanoids\\r\\n8) Nonflavanoid phenols\\r\\n9) Proanthocyanins\\r\\n10)Color intensity\\r\\n11)Hue\\r\\n12)OD280/OD315 of diluted wines\\r\\n13)Proline \\r\\n\\r\\nIn a classification context, this is a well posed problem with \"well behaved\" class structures. A good data set for first testing of a new classifier, but not very challenging.           ', 'purpose': 'test', 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'All attributes are continuous\\r\\n\\t\\r\\nNo statistics available, but suggest to standardise variables for certain uses (e.g. for us with classifiers which are NOT scale invariant)\\r\\n\\r\\nNOTE: 1st attribute is class identifier (1-3)', 'citation': None}}\n",
      "                            name     role         type demographic  \\\n",
      "0                          class   Target  Categorical        None   \n",
      "1                        Alcohol  Feature   Continuous        None   \n",
      "2                      Malicacid  Feature   Continuous        None   \n",
      "3                            Ash  Feature   Continuous        None   \n",
      "4              Alcalinity_of_ash  Feature   Continuous        None   \n",
      "5                      Magnesium  Feature      Integer        None   \n",
      "6                  Total_phenols  Feature   Continuous        None   \n",
      "7                     Flavanoids  Feature   Continuous        None   \n",
      "8           Nonflavanoid_phenols  Feature   Continuous        None   \n",
      "9                Proanthocyanins  Feature   Continuous        None   \n",
      "10               Color_intensity  Feature   Continuous        None   \n",
      "11                           Hue  Feature   Continuous        None   \n",
      "12  0D280_0D315_of_diluted_wines  Feature   Continuous        None   \n",
      "13                       Proline  Feature      Integer        None   \n",
      "\n",
      "   description units missing_values  \n",
      "0         None  None             no  \n",
      "1         None  None             no  \n",
      "2         None  None             no  \n",
      "3         None  None             no  \n",
      "4         None  None             no  \n",
      "5         None  None             no  \n",
      "6         None  None             no  \n",
      "7         None  None             no  \n",
      "8         None  None             no  \n",
      "9         None  None             no  \n",
      "10        None  None             no  \n",
      "11        None  None             no  \n",
      "12        None  None             no  \n",
      "13        None  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "  \n",
    "# fetch dataset \n",
    "wine = fetch_ucirepo(id=109) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = wine.data.features      \n",
    "y = wine.data.targets  \n",
    "df = pd.concat([X, y], axis=1)\n",
    "  \n",
    "# metadata \n",
    "print(wine.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(wine.variables) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Alcohol', 'Malicacid', 'Ash', 'Alcalinity_of_ash', 'Magnesium',\n",
       "       'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols',\n",
       "       'Proanthocyanins', 'Color_intensity', 'Hue',\n",
       "       '0D280_0D315_of_diluted_wines', 'Proline', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 14)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data quality\n",
    "from CustomUtils import DataQualityCheck\n",
    "\n",
    "DataQualityCheck.data_quality_report(input_df=df, type='df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alcohol                         0\n",
       "Malicacid                       0\n",
       "Ash                             0\n",
       "Alcalinity_of_ash               0\n",
       "Magnesium                       0\n",
       "Total_phenols                   0\n",
       "Flavanoids                      0\n",
       "Nonflavanoid_phenols            0\n",
       "Proanthocyanins                 0\n",
       "Color_intensity                 0\n",
       "Hue                             0\n",
       "0D280_0D315_of_diluted_wines    0\n",
       "Proline                         0\n",
       "class                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Initialize ZenML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import pipeline, step\n",
    "from zenml.client import Client\n",
    "from typing import Tuple, Dict, List, Annotated\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from zenml.logger import get_logger\n",
    "import numpy as np\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Initialize ZenML client\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Define ZenML Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def load_data() -> Tuple[\n",
    "    Annotated[pd.DataFrame, \"features\"],\n",
    "    Annotated[pd.DataFrame, \"targets\"]\n",
    "]:\n",
    "    \"\"\"Load the wine dataset.\"\"\"\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "    \n",
    "    wine = fetch_ucirepo(id=109)\n",
    "    X = wine.data.features\n",
    "    y = wine.data.targets\n",
    "    \n",
    "    logger.info(f\"Loaded data - X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "@step\n",
    "def split_data(\n",
    "    X: pd.DataFrame, \n",
    "    y: pd.DataFrame,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 64\n",
    ") -> Tuple[\n",
    "    Annotated[pd.DataFrame, \"X_train\"],\n",
    "    Annotated[pd.DataFrame, \"X_test\"],\n",
    "    Annotated[pd.DataFrame, \"y_train\"],\n",
    "    Annotated[pd.DataFrame, \"y_test\"]\n",
    "]:\n",
    "    \"\"\"Split data into train and test sets.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    logger.info(f\"Split data - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "@step\n",
    "def save_feature_names(X_train: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Save feature names to file and return them.\"\"\"\n",
    "    feature_names = X_train.columns.to_list()\n",
    "    \n",
    "    with open(\"feature_names.txt\", \"w\") as f:\n",
    "        for c in feature_names:\n",
    "            f.write(c + \"\\n\")\n",
    "    \n",
    "    logger.info(f\"Saved {len(feature_names)} feature names\")\n",
    "    return feature_names\n",
    "\n",
    "@step\n",
    "def train_model(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.DataFrame,\n",
    "    n_estimators: int = 100,\n",
    "    random_state: int = 64\n",
    ") -> Tuple[\n",
    "    Annotated[float, \"accuracy\"],\n",
    "    Annotated[Dict, \"params\"]\n",
    "]:\n",
    "    \"\"\"Train a Random Forest model and return accuracy and params.\"\"\"\n",
    "    # Convert y to 1D array if needed\n",
    "    y_train_array = y_train.values.ravel()\n",
    "    y_test_array = y_test.values.ravel()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "    model.fit(X_train, y_train_array)\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test_array, preds)\n",
    "    \n",
    "    params = {\"n_estimators\": n_estimators, \"random_state\": random_state}\n",
    "    \n",
    "    logger.info(f'Accuracy: {acc:.4f}')\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "    \n",
    "    return acc, params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Simple Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mInitiating a new run for the pipeline: \u001b[0m\u001b[38;5;105mtraining_pipeline\u001b[37m.\u001b[0m\n",
      "\u001b[37mUsing user: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37mUsing stack: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37m  deployer: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37m  artifact_store: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37m  orchestrator: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37mYou can visualize your pipeline runs in the \u001b[0m\u001b[38;5;105mZenML Dashboard\u001b[37m. In order to try it locally, please run \u001b[0m\u001b[38;5;105mzenml login --local\u001b[37m.\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105mload_data\u001b[37m has started.\u001b[0m\n",
      "\u001b[37mPreparing to run step \u001b[0m\u001b[38;5;105mload_data\u001b[37m.\u001b[0m\n",
      "\u001b[37mLoaded data - X shape: (178, 13), y shape: (178, 1)\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105mload_data\u001b[37m has finished in \u001b[0m\u001b[38;5;105m2.505s\u001b[37m.\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105msplit_data\u001b[37m has started.\u001b[0m\n",
      "\u001b[37mPreparing to run step \u001b[0m\u001b[38;5;105msplit_data\u001b[37m.\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[37mSplit data - Train: (142, 13), Test: (36, 13)\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105msplit_data\u001b[37m has finished in \u001b[0m\u001b[38;5;105m1.311s\u001b[37m.\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105msave_feature_names\u001b[37m has started.\u001b[0m\n",
      "\u001b[37mPreparing to run step \u001b[0m\u001b[38;5;105msave_feature_names\u001b[37m.\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[37mSaved 13 feature names\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105msave_feature_names\u001b[37m has finished in \u001b[0m\u001b[38;5;105m0.176s\u001b[37m.\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105mtrain_model\u001b[37m has started.\u001b[0m\n",
      "\u001b[37mPreparing to run step \u001b[0m\u001b[38;5;105mtrain_model\u001b[37m.\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[37mAccuracy: 1.0000\u001b[0m\n",
      "Accuracy: 1.0000\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105mtrain_model\u001b[37m has finished in \u001b[0m\u001b[38;5;105m1.208s\u001b[37m.\u001b[0m\n",
      "\u001b[37mPipeline run has finished in \u001b[0m\u001b[38;5;105m10.619s\u001b[37m.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineRunResponse(body=PipelineRunResponseBody(created=datetime.datetime(2026, 1, 7, 13, 48, 5, 662842), updated=datetime.datetime(2026, 1, 7, 13, 48, 16, 362019), user_id=UUID('e0ecb2bb-e896-4c70-a614-0ecb9a68d788'), project_id=UUID('e68f6b36-04e8-4430-b5d8-383ce343a617'), status=<ExecutionStatus.COMPLETED: 'completed'>, in_progress=False, status_reason=None, index=3), metadata=PipelineRunResponseMetadata(run_metadata={}, config=PipelineConfiguration(enable_cache=None, enable_artifact_metadata=None, enable_artifact_visualization=None, enable_step_logs=None, environment={}, secrets=[], enable_pipeline_logs=None, execution_mode=<ExecutionMode.CONTINUE_ON_FAILURE: 'continue_on_failure'>, settings={}, tags=None, extra={}, failure_hook_source=None, success_hook_source=None, init_hook_source=None, init_hook_kwargs=None, cleanup_hook_source=None, model=None, parameters={'test_size': 0.2, 'n_estimators': 100, 'random_state': 64}, retry=None, substitutions={'date': '2026_01_07', 'time': '13_48_05_661082'}, cache_policy=None, name='training_pipeline'), start_time=datetime.datetime(2026, 1, 7, 13, 48, 5, 661082), end_time=datetime.datetime(2026, 1, 7, 13, 48, 15, 809002), client_environment={'environment': 'notebook', 'os': 'windows', 'windows_version_release': '10', 'windows_version': '10.0.26200', 'windows_version_service_pack': 'SP0', 'windows_version_os_type': 'Multiprocessor Free', 'python_version': '3.10.11'}, orchestrator_environment={'environment': 'notebook', 'os': 'windows', 'windows_version_release': '10', 'windows_version': '10.0.26200', 'windows_version_service_pack': 'SP0', 'windows_version_os_type': 'Multiprocessor Free', 'python_version': '3.10.11'}, orchestrator_run_id='5609aa0f-f0d0-469f-8aa8-2d21a6dcd3f9', code_path=None, template_id=None, is_templatable=False, trigger_info=None), resources=PipelineRunResponseResources(user=UserResponse(body=UserResponseBody(created=datetime.datetime(2026, 1, 7, 13, 28, 56, 331295), updated=datetime.datetime(2026, 1, 7, 13, 41, 13, 936593), active=True, activation_token=None, full_name='Aash', email_opted_in=False, is_service_account=False, is_admin=True, default_project_id=None, avatar_url=None), metadata=None, resources=None, id=UUID('e0ecb2bb-e896-4c70-a614-0ecb9a68d788'), permission_denied=False, name='default'), snapshot=PipelineSnapshotResponse(body=PipelineSnapshotResponseBody(created=datetime.datetime(2026, 1, 7, 13, 48, 5, 606971), updated=datetime.datetime(2026, 1, 7, 13, 48, 5, 606971), user_id=UUID('e0ecb2bb-e896-4c70-a614-0ecb9a68d788'), project_id=UUID('e68f6b36-04e8-4430-b5d8-383ce343a617'), runnable=False, deployable=False, is_dynamic=False), metadata=None, resources=None, id=UUID('5a64ffeb-b109-4114-9834-58d1ea4d555b'), permission_denied=False, name=None), source_snapshot=None, stack=StackResponse(body=StackResponseBody(created=datetime.datetime(2026, 1, 7, 13, 28, 55, 646257), updated=datetime.datetime(2026, 1, 7, 13, 28, 55, 646257), user_id=None), metadata=None, resources=None, id=UUID('fc921835-702c-46dd-832a-f4cab5fbe3b1'), permission_denied=False, name='default'), pipeline=PipelineResponse(body=PipelineResponseBody(created=datetime.datetime(2026, 1, 7, 13, 45, 15, 420296), updated=datetime.datetime(2026, 1, 7, 13, 45, 15, 420296), user_id=UUID('e0ecb2bb-e896-4c70-a614-0ecb9a68d788'), project_id=UUID('e68f6b36-04e8-4430-b5d8-383ce343a617')), metadata=None, resources=None, id=UUID('d0cd049e-c56f-4b7d-aad2-71aabed3cf66'), permission_denied=False, name='training_pipeline'), build=None, schedule=None, code_reference=None, trigger_execution=None, model_version=None, tags=[], log_collection=[LogsResponse(body=LogsResponseBody(created=datetime.datetime(2026, 1, 7, 13, 48, 5, 704061), updated=datetime.datetime(2026, 1, 7, 13, 48, 5, 704061), uri='C:\\\\Users\\\\Adarsh\\\\AppData\\\\Roaming\\\\zenml\\\\local_stores\\\\2b5da72e-9732-40cc-8458-cb675cb98fee\\\\logs\\\\942a4777-b7f3-43c7-9bb5-c6e90b217090.log', source='client'), metadata=None, resources=None, id=UUID('942a4777-b7f3-43c7-9bb5-c6e90b217090'), permission_denied=False)], visualizations=[]), id=UUID('0b4d661d-cea6-4878-9468-fa2ea827a4aa'), permission_denied=False, name='training_pipeline-2026_01_07-13_48_05_661082')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@pipeline\n",
    "def training_pipeline(\n",
    "    test_size: float = 0.2,\n",
    "    n_estimators: int = 100,\n",
    "    random_state: int = 64\n",
    "):\n",
    "    \"\"\"Simple training pipeline.\"\"\"\n",
    "    # Load data\n",
    "    X, y = load_data()\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(\n",
    "        X=X, \n",
    "        y=y, \n",
    "        test_size=test_size, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Save feature names\n",
    "    feature_names = save_feature_names(X_train=X_train)\n",
    "    \n",
    "    # Train model\n",
    "    accuracy, params = train_model(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "# Run the pipeline\n",
    "training_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Grid Search Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mInitiating a new run for the pipeline: \u001b[0m\u001b[38;5;105mgrid_search_pipeline\u001b[37m.\u001b[0m\n",
      "\u001b[37mRegistered new pipeline: \u001b[0m\u001b[38;5;105mgrid_search_pipeline\u001b[37m.\u001b[0m\n",
      "\u001b[37mUsing user: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37mUsing stack: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37m  deployer: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37m  artifact_store: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37m  orchestrator: \u001b[0m\u001b[38;5;105mdefault\u001b[37m\u001b[0m\n",
      "\u001b[37mYou can visualize your pipeline runs in the \u001b[0m\u001b[38;5;105mZenML Dashboard\u001b[37m. In order to try it locally, please run \u001b[0m\u001b[38;5;105mzenml login --local\u001b[37m.\u001b[0m\n",
      "\u001b[37mUsing cached version of step \u001b[0m\u001b[38;5;105mload_data\u001b[37m.\u001b[0m\n",
      "\u001b[37mUsing cached version of step \u001b[0m\u001b[38;5;105msplit_data\u001b[37m.\u001b[0m\n",
      "\u001b[37mUsing cached version of step \u001b[0m\u001b[38;5;105msave_feature_names\u001b[37m.\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105mgrid_search_train\u001b[37m has started.\u001b[0m\n",
      "\u001b[37mPreparing to run step \u001b[0m\u001b[38;5;105mgrid_search_train\u001b[37m.\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[33mBy default, the \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m stores data as a \u001b[0m\u001b[38;5;105m.csv\u001b[33m file. If you want to store data more efficiently, you can install \u001b[0m\u001b[38;5;105mpyarrow\u001b[33m by running '\u001b[0m\u001b[38;5;105mpip install pyarrow\u001b[33m'. This will allow \u001b[0m\u001b[38;5;105mPandasMaterializer\u001b[33m to automatically store the data as a \u001b[0m\u001b[38;5;105m.parquet\u001b[33m file instead.\u001b[0m\n",
      "\u001b[33md:\\Projects\\Coding\\academic\\MLOps\\Labs\\ZenML Demo\\.venv\\lib\\site-packages\\zenml\\integrations\\pandas\\materializers\\pandas_materializer.py:144: UserWarning: Could not infer format, so each element will be parsed individually, falling back to \u001b[0m\u001b[38;5;105mdateutil\u001b[33m. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(f, index_col=0, parse_dates=True)\n",
      "\u001b[0m\n",
      "\u001b[37mModel 1: n_estimators=72, test_acc=1.0000, cv_mean=0.9857\u001b[0m\n",
      "\u001b[37mModel 2: n_estimators=100, test_acc=1.0000, cv_mean=0.9857\u001b[0m\n",
      "\u001b[37mModel 3: n_estimators=125, test_acc=1.0000, cv_mean=0.9857\u001b[0m\n",
      "\u001b[37mModel 4: n_estimators=150, test_acc=1.0000, cv_mean=0.9857\u001b[0m\n",
      "\u001b[37mModel 5: n_estimators=200, test_acc=1.0000, cv_mean=0.9857\u001b[0m\n",
      "\u001b[37mModel 6: n_estimators=250, test_acc=1.0000, cv_mean=0.9857\u001b[0m\n",
      "\u001b[37mBest model: {'n_estimators': 72}, accuracy: 1.0000\u001b[0m\n",
      "\u001b[37mStep \u001b[0m\u001b[38;5;105mgrid_search_train\u001b[37m has finished in \u001b[0m\u001b[38;5;105m11.776s\u001b[37m.\u001b[0m\n",
      "\u001b[37mPipeline run has finished in \u001b[0m\u001b[38;5;105m13.248s\u001b[37m.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PipelineRunResponse(body=PipelineRunResponseBody(created=datetime.datetime(2026, 1, 7, 13, 48, 41, 652271), updated=datetime.datetime(2026, 1, 7, 13, 48, 55, 392691), user_id=UUID('e0ecb2bb-e896-4c70-a614-0ecb9a68d788'), project_id=UUID('e68f6b36-04e8-4430-b5d8-383ce343a617'), status=<ExecutionStatus.COMPLETED: 'completed'>, in_progress=False, status_reason=None, index=1), metadata=PipelineRunResponseMetadata(run_metadata={}, config=PipelineConfiguration(enable_cache=None, enable_artifact_metadata=None, enable_artifact_visualization=None, enable_step_logs=None, environment={}, secrets=[], enable_pipeline_logs=None, execution_mode=<ExecutionMode.CONTINUE_ON_FAILURE: 'continue_on_failure'>, settings={}, tags=None, extra={}, failure_hook_source=None, success_hook_source=None, init_hook_source=None, init_hook_kwargs=None, cleanup_hook_source=None, model=None, parameters={'test_size': 0.2, 'random_state': 64}, retry=None, substitutions={'date': '2026_01_07', 'time': '13_48_41_643687'}, cache_policy=None, name='grid_search_pipeline'), start_time=datetime.datetime(2026, 1, 7, 13, 48, 41, 643687), end_time=datetime.datetime(2026, 1, 7, 13, 48, 54, 814277), client_environment={'environment': 'notebook', 'os': 'windows', 'windows_version_release': '10', 'windows_version': '10.0.26200', 'windows_version_service_pack': 'SP0', 'windows_version_os_type': 'Multiprocessor Free', 'python_version': '3.10.11'}, orchestrator_environment={'environment': 'notebook', 'os': 'windows', 'windows_version_release': '10', 'windows_version': '10.0.26200', 'windows_version_service_pack': 'SP0', 'windows_version_os_type': 'Multiprocessor Free', 'python_version': '3.10.11'}, orchestrator_run_id='6595b696-187d-455b-9b54-123264c75674', code_path=None, template_id=None, is_templatable=False, trigger_info=None), resources=PipelineRunResponseResources(user=UserResponse(body=UserResponseBody(created=datetime.datetime(2026, 1, 7, 13, 28, 56, 331295), updated=datetime.datetime(2026, 1, 7, 13, 41, 13, 936593), active=True, activation_token=None, full_name='Aash', email_opted_in=False, is_service_account=False, is_admin=True, default_project_id=None, avatar_url=None), metadata=None, resources=None, id=UUID('e0ecb2bb-e896-4c70-a614-0ecb9a68d788'), permission_denied=False, name='default'), snapshot=PipelineSnapshotResponse(body=PipelineSnapshotResponseBody(created=datetime.datetime(2026, 1, 7, 13, 48, 41, 542109), updated=datetime.datetime(2026, 1, 7, 13, 48, 41, 542109), user_id=UUID('e0ecb2bb-e896-4c70-a614-0ecb9a68d788'), project_id=UUID('e68f6b36-04e8-4430-b5d8-383ce343a617'), runnable=False, deployable=False, is_dynamic=False), metadata=None, resources=None, id=UUID('06aabbc9-5ac0-47a8-a796-62603fa95eb4'), permission_denied=False, name=None), source_snapshot=None, stack=StackResponse(body=StackResponseBody(created=datetime.datetime(2026, 1, 7, 13, 28, 55, 646257), updated=datetime.datetime(2026, 1, 7, 13, 28, 55, 646257), user_id=None), metadata=None, resources=None, id=UUID('fc921835-702c-46dd-832a-f4cab5fbe3b1'), permission_denied=False, name='default'), pipeline=PipelineResponse(body=PipelineResponseBody(created=datetime.datetime(2026, 1, 7, 13, 48, 40, 972309), updated=datetime.datetime(2026, 1, 7, 13, 48, 40, 972309), user_id=UUID('e0ecb2bb-e896-4c70-a614-0ecb9a68d788'), project_id=UUID('e68f6b36-04e8-4430-b5d8-383ce343a617')), metadata=None, resources=None, id=UUID('255a8982-0927-4db5-bc80-0adfe045b269'), permission_denied=False, name='grid_search_pipeline'), build=None, schedule=None, code_reference=None, trigger_execution=None, model_version=None, tags=[], log_collection=[LogsResponse(body=LogsResponseBody(created=datetime.datetime(2026, 1, 7, 13, 48, 41, 685158), updated=datetime.datetime(2026, 1, 7, 13, 48, 41, 685158), uri='C:\\\\Users\\\\Adarsh\\\\AppData\\\\Roaming\\\\zenml\\\\local_stores\\\\2b5da72e-9732-40cc-8458-cb675cb98fee\\\\logs\\\\959e3021-897a-4493-98f6-b01f38af68f7.log', source='client'), metadata=None, resources=None, id=UUID('959e3021-897a-4493-98f6-b01f38af68f7'), permission_denied=False)], visualizations=[]), id=UUID('ba8f4a3b-1d5d-45a6-a69f-0f1fb1a2a28c'), permission_denied=False, name='grid_search_pipeline-2026_01_07-13_48_41_643687')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@step\n",
    "def grid_search_train(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.DataFrame,\n",
    "    n_estimators_list: List[int]\n",
    ") -> Tuple[\n",
    "    Annotated[float, \"best_accuracy\"],\n",
    "    Annotated[Dict, \"best_params\"],\n",
    "    Annotated[List[Dict], \"all_results\"]\n",
    "]:\n",
    "    \"\"\"Perform grid search and return best model info and all results.\"\"\"\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    # Convert y to 1D array\n",
    "    y_train_array = y_train.values.ravel()\n",
    "    y_test_array = y_test.values.ravel()\n",
    "    \n",
    "    param_grid = {\"n_estimators\": n_estimators_list}\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=64, n_jobs=-1),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train_array)\n",
    "    \n",
    "    all_results = []\n",
    "    best_acc = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    # Iterate over each candidate\n",
    "    for i in range(len(grid.cv_results_[\"params\"])):\n",
    "        params = grid.cv_results_[\"params\"][i]\n",
    "        mean_val = grid.cv_results_[\"mean_test_score\"][i]\n",
    "        std_val = grid.cv_results_[\"std_test_score\"][i]\n",
    "        \n",
    "        # Build & refit the model manually for logging\n",
    "        model = RandomForestClassifier(**params, random_state=64, n_jobs=-1)\n",
    "        model.fit(X_train, y_train_array)\n",
    "        \n",
    "        preds = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test_array, preds)\n",
    "        \n",
    "        result = {\n",
    "            \"params\": params,\n",
    "            \"cv_mean_accuracy\": float(mean_val),\n",
    "            \"cv_std_accuracy\": float(std_val),\n",
    "            \"test_accuracy\": float(acc)\n",
    "        }\n",
    "        all_results.append(result)\n",
    "        \n",
    "        logger.info(f\"Model {i+1}: n_estimators={params['n_estimators']}, \"\n",
    "                   f\"test_acc={acc:.4f}, cv_mean={mean_val:.4f}\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_params = params\n",
    "    \n",
    "    logger.info(f\"Best model: {best_params}, accuracy: {best_acc:.4f}\")\n",
    "    \n",
    "    return best_acc, best_params, all_results\n",
    "\n",
    "@pipeline\n",
    "def grid_search_pipeline(\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 64\n",
    "):\n",
    "    \"\"\"Grid search training pipeline.\"\"\"\n",
    "    # Load data\n",
    "    X, y = load_data()\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Save feature names\n",
    "    feature_names = save_feature_names(X_train=X_train)\n",
    "    \n",
    "    # Grid search\n",
    "    n_estimators_list = [72, 100, 125, 150, 200, 250]\n",
    "    best_acc, best_params, all_results = grid_search_train(\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        n_estimators_list=n_estimators_list\n",
    "    )\n",
    "\n",
    "# Run the grid search pipeline\n",
    "grid_search_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['Alcohol', 'Malicacid', 'Ash', 'Alcalinity_of_ash', 'Magnesium', 'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins', 'Color_intensity', 'Hue', '0D280_0D315_of_diluted_wines', 'Proline']\n"
     ]
    }
   ],
   "source": [
    "# Load feature names\n",
    "with open(\"feature_names.txt\") as f:\n",
    "    feature_names = [line.strip() for line in f]\n",
    "\n",
    "print(f\"Feature names: {feature_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## View ZenML Dashboard\n",
    "\n",
    "\n",
    "\n",
    " To view your pipeline runs, models, and artifacts:\n",
    "\n",
    "\n",
    "\n",
    " ```bash\n",
    "\n",
    " zenml up\n",
    "\n",
    " ```\n",
    "\n",
    "\n",
    "\n",
    " The dashboard will show:\n",
    "\n",
    " - All pipeline runs with their steps\n",
    "\n",
    " - Artifacts (datasets, models, metrics)\n",
    "\n",
    " - Model versions and metadata\n",
    "\n",
    " - Lineage tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Model Deployment\n",
    "\n",
    "\n",
    "\n",
    " For production deployment with ZenML, you can use various deployment integrations:\n",
    "\n",
    "\n",
    "\n",
    " ```python\n",
    "\n",
    " # Example: Deploy with MLflow (requires mlflow integration)\n",
    "\n",
    " from zenml.integrations.mlflow.steps import mlflow_model_deployer_step\n",
    "\n",
    "\n",
    "\n",
    " @pipeline\n",
    "\n",
    " def deployment_pipeline():\n",
    "\n",
    "     # ... training steps ...\n",
    "\n",
    "     mlflow_model_deployer_step(\n",
    "\n",
    "         model=trained_model,\n",
    "\n",
    "         deploy_decision=True\n",
    "\n",
    "     )\n",
    "\n",
    " ```\n",
    "\n",
    "\n",
    "\n",
    " Or use other deployers:\n",
    "\n",
    " - Seldon Core\n",
    "\n",
    " - KServe\n",
    "\n",
    " - BentoML\n",
    "\n",
    " - Custom deployers\n",
    "\n",
    "\n",
    "\n",
    " Install integration: `zenml integration install mlflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
